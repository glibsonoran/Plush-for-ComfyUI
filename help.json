{
    "sp_help": "\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\n****************\n\n\n\u2726 AI_Selection [input connection]: Attach the Plush 'AI_Chooser' Node to this input so you can select the AI_Service and model you want to use.  As of v1.21.11 ChatGPT, Anthropic & Groq services and models are available. \n\n\u2726 creative_latitude:  Higher numbers give the model more freedom to interpret your prompt or image.  Lower numbers constrain the model to stick closely to your input.\n\n\u2726 tokens: A limit on how many tokens are made available for ChatGPT to use, it doesn't have to use them all.\n\n\u2726 style: Choose the art style you want to base your prompt on.  If this list is too long, type a few characters of the style you're looking for and the list will dynamically filter.\n\n\u2726 artist: Will produce a 'style of' phrase listing the number of artists you indicate.  They will be artists that work in the chosen style.  Choose 0 if you don't want this.\n\n\u2726 prompt_style: 'Narrative' is long form grammatically correct creative writing, This is the preferred form for Dall-e. 'Tags' is a terse, stripped down list of visual attributes without grammatical phrasing, This is the preferred form for SD and Midjourney.\n\n\u2726 max_elements: A limit on the number of distinct descriptions of visual elements in the prompt. Smaller numbers makes a shorter prompt.\n\n\u2726 style_info: Set to True if you want background information about the art style you chose.",
    "wrangler_help": "\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\u2022 Exif Wrangler will extract Exif and/or AI generation workflow metadata from .jpg (.jpeg) and .png images.  .jpg photographs can be queried for their camera settings.  ComfyUI's .png files will yield certain values from their workflow including the prompt, seed etc.  Images from other AI generators may or may not yield data depending on where they store their metadata. For instance Auto 1111 .jpg's will yield their workflow information that's stored in their Exif comment.\n\n**************\n  \n\u2726 write_to_file: Whether or not to save the meta data file you see in the output to a .txt file in the: '.../ComfyUI/output/PlushFiles' directory.\n\n\u2726 file_prefix: The prefix for the file name of the saved file, this will be appended to a date/time value to make the file unique. The file will have a .txt extension: e.g., 'MyFileName_ew_20240204_193224.txt'\n\n\u2726 Min_Prompt_len:  A filter value for prompts: Exif Wrangler has to distinguish between actual prompts and other long strings in the ComfyUI embeded meta data.  Every Note, every text display box, and even some text that's hidden in nodes is included in the JSON that holds this information.  This field allows you to set a minimum length for strings to be displayed to help filter out shorter unwanted text strings.\n\n\u2726 Alpha_Char_Pct: Another prompt filter that works by only allowing text strings that have a percentage of alpha ASCII characters (Aa - Zz plus comma) equal to or higher than this setting.  Increasing the percentage screens out strings that have lots of bytes, symbols and numbers.  If you use a lot of weightings or Lora values in your prompts that introduce angle brackets, parentheses, brackets and colons, you may have to lower this percentage to see your prompt.  \n\n\u2726 Prompt_Filter_Term:  Enter a single term or short phrase here. A particular prompt string will only be included in Possible Prompts if it contains an exact match for this term.  This can be used in a couple of ways:  \n 1) If you know there's a term you always or frequently use in the prompts, or if you remember part of a particular image prompt's wording,  you can add it here before you click the Queue button.  \n 2) If, after clicking Queue, a lot of Possible Prompt candidates clutter your output.  Find the one you know is the actual prompt, find a unique word or phrase in it e.g.: 'regal'.  Enter that word or phrase as a filter term and run Wrangler again.  You'll get back an uncluttered response to save as a file.\n\n***************\n\n\u2726 troubleshooting output:  Hook this output up to a text display node to see any INFO/WARNING/ERROR data that's generated during this node's run. ",
	"dalle_help": "\u2022  Use 'Show Text|pysssss' nodes for displaying text output from Plush nodes.  Plush outputs text as UTF-8 Unicode, which Show Text can display correctly.\n\n\u2022 Dall-e Image will produce an image .PNG from a text prompt using the Dall-e 3 model from OpenAI. It requires a OpenAI API key.\n\n**************\n\n\u2726 GPTmodel: The Dall-e model that will generate the image file.  Currently this is limited to Dall-e 3.\n\n\u2726 prompt: The text prompt for the image you want to produce.  Be aware that OpenAI will generate their own prompt from your prompt and pass that to the image model.\n\n\u2726 image_size: Choose a square, portrait or landscape image.  The image size format is: Width, Height.  The 1792 image sizes cost slightly more tokens.\n\n\u2726 image_quality: Self explanatory, you can experiment to see if you think there's a noticable difference.  The standard quality image costs a few less tokens than hd.\n\n\u2726 style: Vivid produces a little more contrast and more saturated colors.  The choice depends on what type of image you're trying to produce.\n\n\u2726  batch_size:  The number of images you want to produce in one run.  The vast majority of the times batches run without incident, but you should be aware that sending image requests to the Dall-e server is not as reliable as running images locally in SD.  If the server gets overtaxed, or hiccups you may not get back all the images you requested. This Dall-e node will handle OpenAI server errors gracefully and allow your batch to continue to completion, but sometimes you may get back fewer images than you requested.  If you keep the 'troubleshooting' output connected it will report any errors and let you know how many images were processed vs how many you requested.\n\n\u2726  seed:  This works just like a seed in a KSampler except that it doesn't affect a latent or the image.  It's simply there for you to set to: 'randomize' or 'increment' if you want Dall-e to run with every Queue, or to 'fixed' if you only want Dall-e to run once per prompt or setting.  The Dall_e API doesn't actually pass seed values.  This can also be controlled by the 'Global Seed' from the Inspire Pack. \n\n***************\n\n\u2726 troubleshooting output:  Hook this output up to a text display node to see any INFO/WARNING/ERROR data that's generated during this node's run.\n\n\u2726  Dalle_e_prompt: The prompt that Dall-e 3 generates from your prompt.  This is the prompt that actually gets passed to the image model.  Hook up a text display node to see it.",
	"adv_prompt_help": "\u2022 Advanced Prompt Enhancer (APE) uses AI Models to generate text output from any combination of: Instruction, Example_or_Context, Image and Prompt you provide. No API key is needed for Open source Models.  This node can use various remote models, ChatGPT, Groq and Anthropic Claude if you have an API key and have stored it in an environment variable (see ReadMe file).  With or without a key it can also connect to various local apps and models e.g.: LM Studio, Oobabooga, Koboldcpp, etc.\n\n\u2022 image input: Advanced Prompt Enhancer can send image data (in the form of a b64 image file) to AI vision capable models.  If you're sending an image to an AI model be sure both the model and the app or remote service have vision capabilities and can handle image files.\n\n\u2022 Examples_or_Context: APE can send example(s) and/or context along with your instructions to the LLM.  Examples and Context *always* need to be in the form of: User input, then the delimiter, followed by the model's response.  Delimited text entered in this field will automatically create alternatating input to the model for each delimited segment using this pattern.  If you want to explicitly tag your text as being user or model input you can preface each delimited segment with {{user}} or {{model}}. (There's an workflow file: 'How_To_Use_Examples.png' in the 'Example_Worflows' folder with details about using the Examples_or_Context input.)\n\n\u2022 API Keys:  API keys need to be kept in environment variables.  The Environment Variable names that Advanced Prompt Enhancer looks for are: \u2726ChatGPT: OPENAI_API_KEY or OAI_KEY;  \u2726Groq: GROQ_API_KEY;  \u2726Anthropic: ANTHROPIC_API_KEY.  Find instructions on how to create the Enviroment Variable here: https://github.com/glibsonoran/Plush-for-ComfyUI?tab=readme-ov-file#requirements .  \n\n**************\n\n\u2022  AI_service: This indicates the type of AI service and connection you're going to send your data to.  If you're using a local AI app you'll need to provide a valid URL in the LLM_URL field near the bottom of the node.  If you're using 'Oobabooga API' make sure you read the LLM_URL help below.  'OpenAI compatible http POST' uses a web POST action rather than the OpenAI API Object to communicate with the local or remote AI server, typically this requires a 'v1/chat/completions' path in the URL. 'http POST Simplified Data' also uses a web POST action and presents a simplified data structure. Try this if the other AI service methods don't work, it will also require a: 'v1/chat/completions' path.  \n\n\u2022 GPTmodel: This field only applies when the LLM field is set to 'ChatGPT'.  Select the specific OpenAI ChatGPT model you want to use.  If you're inputting an image, make sure the model you choose is vision capable.\n\n\u2022 Groq_model: This only applies when you select 'Groq' in the AI_service field. Choose the Groq model you want to use. \n\n\u2022 Anthropic_model: This only applies when you select 'Anthropic' from the AI_service field.  Choose the Anthopic model you want to use. \n\n\u2022 Ollama_model: This will display the model(s) currently loaded in the Ollama front end. In order for models to show up in the drop down Ollama will have to be running with the models you intend to use loaded *before* starting ComfyUI. Note that APE looks for the standard url: http://localhost:11434/api/tags when retrieving the model names.  If you've setup Ollama with another url (e.g. different port), you'll need to modify the 'urls.json' file. \n\n\u2022 creative_latitude: (Temperature)  This will set how strictly the LLM adheres to common word relationships and how closely it will follow your instruction and prompt.  Setting this value higher allows more creative freedom in interpreting your input and generating its ouptput.\n\n\u2022 tokens:  The maximum number of tokens that the LLM can use in processing your prompt and return text.  This is not the number of tokens  it 'will' use, it's the number available that it 'can' use.\n\n\u2022 seed: This is a pseudo or mock seed, it has no effect on the text generated, and it's not passed to the LLM.  It's used here solely to control when the node will run.  It works the same as a KSampler, set it to 'fixed' if you want the node to run only once each time you change your inputs, set it to random or increment/decrement if you want it run with each Queue.\n\n\u2022 example_delimiter: You can provide multiple examples or context to the LLM.  Providing multiple examples for a given instruction is a type of 'Few Shot Prompting', which can be effective with some LLM's. This field indicates how the node will distinguish each separate example, each separate example or context item will be presented as originating from the User then the Model alternating in that order for as many as you enter.  You can choose to separate your examples with a pipe '|' character, two newlines (i.e.: carriage returns) or two colons '::', these are called delimiters and they denote where these separations will occur.\n\n\u2022 LLM_URL: When using an LLM other than ChatGPT, Anthropic or Groq you'll need to provide a URL in this field.  Typically the AI application you're using (e.g. LM Studio, Oobabooga), will indicate the URL to use after you startup its server. It may be in the terminal output or in the UI. Some local AI apps will specify that a particular URL is OpenAI compatible, if so this is the one you want to use.  Typically the URLs for local apps have this general format: http://localhost:5001/v1 where '5001' is the port and 'localhost' is interchangable with '127.0.0.1'.  If you're using the Oobabooga API or 'OpenAI compatible http POST' selection your url will need to have /chat/completions appended as part of the url: http://127.0.0.1:5000/v1/chat/completions\n\n**************\n\n\u2022 Use the troubleshooting output if you have issues with model connections, or if you want to see exactly which model was used to produce your output (some ChatGPT model names are actually only pointers to the latest specific model in that category) and how many tokens were used.",
	"tagger_help": "\u2022 Tagger adds tags to the beginning, middle or end of a text block.  Tagger can be used whenever you want to add text that needs to appear exactly as written. \n\n**************\n\n\u2022 Beginning_tags: The text (tags) you want to appear at the very beginning of the input text block.  It will preface all other text in the block. \n\n\u2022 Middle_tags:  The text (tags) you want to appear in the middle of the text block.  These tags will always appear immediately after a comma or period.  \n\n\u2022 Prefer_middle_tag_after_period: You can indicate a preference for the tags to follow a period by clicking this button.  Otherwise the tags may follow a period or a comma whichever is closest to the middle of the text. \n\n\u2022 End_tags:  Tags that will be appended to the end of the input text block.\n\n\u2022  Examples:  Beginning_tags: '[An Abstract Painting:| Digital Art:]', Middle_tags: '(Big Black Hat:1.4)', End_tags: 'In the style of Piet Mondrian' " 
}